{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11675351,"sourceType":"datasetVersion","datasetId":7327597}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 32\nblock_size = 8\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\n\n\n# ------------\n\ntorch.manual_seed(2023)\n\n# Load the Victor Hugo dataset\nwith open('/kaggle/input/data-training-gpt2/hugo_contemplations.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n\n\n    def forward(self, x):\n        B, T, C = x.shape\n\n        k = self.key(x)       # (B, T, head_size)\n        q = self.query(x)     # (B, T, head_size)\n        v = self.value(x)     # (B, T, head_size)\n\n        # Compute attention scores\n        weights = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        weights = F.softmax(weights, dim=-1)         # (B, T, T)\n\n        # Apply attention weights to values\n        out = weights @ v                            # (B, T, head_size)\n        return out\n\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.sa_head(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n\n# generate from the model\nprompt = torch.tensor(encode(['\\n']), device=device)\ncontext = torch.ones((1, 1), dtype=torch.long, device=device) * prompt\n\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))","metadata":{"_uuid":"9a072276-d7aa-44a3-9b28-1a8b1f6b2fbe","_cell_guid":"def05d1f-37cb-4a6d-9181-efbe97ffc2f6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-04T17:01:39.543209Z","iopub.execute_input":"2025-05-04T17:01:39.543511Z","iopub.status.idle":"2025-05-04T17:02:00.183568Z","shell.execute_reply.started":"2025-05-04T17:01:39.543490Z","shell.execute_reply":"2025-05-04T17:02:00.182916Z"}},"outputs":[{"name":"stdout","text":"0.009893 M parameters\nstep 0: train loss 4.6812, val loss 4.6833\nstep 500: train loss 2.7358, val loss 2.8455\nstep 1000: train loss 2.4925, val loss 2.5880\nstep 1500: train loss 2.4397, val loss 2.5376\nstep 2000: train loss 2.3942, val loss 2.5404\nstep 2500: train loss 2.3766, val loss 2.5541\nstep 3000: train loss 2.3624, val loss 2.5050\nstep 3500: train loss 2.3424, val loss 2.4783\nstep 4000: train loss 2.3457, val loss 2.3882\nstep 4500: train loss 2.3352, val loss 2.4472\nstep 4999: train loss 2.3324, val loss 2.4422\n\n emancée vul le pupreu vele!\n tt bre, fones norghe me  ssutombanche?\nLencore, faulesuse fîbleux, end qu'ans on.\n mbri voi que encr,\nL'erchanc l'à luraurr.\nSitét de quies dommes e es hère  d'es  hema, le von! hèrerièret ndens\n à  étu antux, a t rit, voule eisi tt des, tan-t se, heut lutentes n à hèvouieux,\nPaoù achen!\nPeu ler  fan vobren preande pr leu nevracablent deul e ge,\nL'oune chire.\nEque,\nOyi, meut   âpoure denourchbegll'étandes, chis, rà brtarves ffotit et e c suce\nVointe, qu à c t fut da\n","output_type":"stream"}],"execution_count":11}]}